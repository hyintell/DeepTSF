# DeepTSF

**DeepTSF** is an open software package and collection for time series forecasting with deep learning models.
It contains a variety of models, from basic models such as RNNs to more complex deep neural networks such as transformers and GNNs.

* Free software: MIT

## Features

- Transformers
- Graph Neural Networks
- Univariate model and multivariate model
- Point estimation and interval estimation
- Unified modelling with Multiple items


## Models list

| Model        |        Paper                            |
|--------------|-----------------------------------------|
| DARNN      |   [A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction](https://arxiv.org/pdf/1704.02971.pdf)                                      |
| FEDformer      |   [FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting](https://arxiv.org/pdf/2201.12740.pdf)                                      |
| LTSF-Linear      |   [Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/pdf/2205.13504.pdf)                                      |
| DeepAR       |[DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks](https://arxiv.org/abs/1704.04110)                                         |
| MTGNN       |[Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks](https://arxiv.org/pdf/2005.11650.pdf)                                         |
| PYRAFORMER       |[PYRAFORMER: LOW-COMPLEXITY PYRAMIDAL ATTENTION FOR LONG-RANGE TIME SERIES MODELING AND FORECASTING](https://openreview.net/pdf?id=0EXmFzUn5I)                                         |
| Lstnet       |[Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks](https://arxiv.org/pdf/1703.07015.pdf)                                         |
| MQ-RNN       |  [A Multi-Horizon Quantile Recurrent Forecaster](https://arxiv.org/pdf/1711.11053.pdf)                                       |
| N-Beats      | [N-BEATS: Neural basis expansion analysis for interpretable time series forecasting](https://arxiv.org/abs/1905.10437)                                |
| TCN          |  [An empirical evaluation of generic convolutional and recurrent networks for sequence modeling](https://arxiv.org.1803.01271)                                     |
| Transformer  |    [Attention Is All You Need](https://arxiv.org/abs/1706.03762)                                     |
| Informer     |[Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436)                                         |
| Autoformer   | [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008)                                        |
| TFT          | [Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting](https://arxiv.org/pdf/1912.09363.pdf)                                        |
| MAE          |  [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/pdf/2111.06377.pdf)                                       |
| Transformer          |  [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/pdf/2111.06377.pdf)                                       |

## LICENSE

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgements


